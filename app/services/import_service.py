"""
–°–µ—Ä–≤–∏—Å –∏–º–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö –æ —Ç–æ–≤–∞—Ä–∞—Ö
"""
import pandas as pd
import json
import csv
from pathlib import Path
from app import db
from app.models.product import Product, ProductStatus
from app.models.subcategory import Subcategory
from app.models.attribute import Attribute, AttributeType
from app.models.subcategory_attribute import SubcategoryAttribute
from app.models.product import ProductAttributeValue
from app.models.workflow import ProductStatusHistory
from app.services.verification_service import VerificationService
from flask_login import current_user
from datetime import datetime

class ImportService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ —Ñ–∞–π–ª–æ–≤"""
    
    @staticmethod
    def import_from_file(file_path, subcategory_id, user=None, auto_verify=True, import_history_id=None):
        """
        –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–≤–∞—Ä—ã –∏–∑ —Ñ–∞–π–ª–∞
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            subcategory_id: ID –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            user: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å, –≤—ã–ø–æ–ª–Ω—è—é—â–∏–π –∏–º–ø–æ—Ä—Ç
            auto_verify: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø—É—Å–∫–∞—Ç—å –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é –ø–æ—Å–ª–µ –∏–º–ø–æ—Ä—Ç–∞
            import_history_id: ID –∑–∞–ø–∏—Å–∏ ImportHistory (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
        Returns:
            dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–º–ø–æ—Ä—Ç–∞
        """
        file_path = Path(file_path)
        file_extension = file_path.suffix.lower()
        
        # –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ª–∏—Å—Ç–∞
        subcategory = Subcategory.query.get(subcategory_id)
        subcategory_name = subcategory.name if subcategory else None
        
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ –∏ –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
        if file_extension in ['.xlsx', '.xls']:
            data, total_rows = ImportService._parse_excel(file_path, subcategory_name=subcategory_name)
        elif file_extension == '.csv':
            data, total_rows = ImportService._parse_csv(file_path)
        elif file_extension == '.json':
            data, total_rows = ImportService._parse_json(file_path)
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_extension}")
        
        # –í—ã–ø–æ–ª–Ω–∏—Ç—å –∏–º–ø–æ—Ä—Ç
        result = ImportService._import_products(data, subcategory_id, user, auto_verify, import_history_id)
        result['total_rows'] = total_rows
        return result
    
    @staticmethod
    def _parse_excel(file_path, sheet_name=None, subcategory_name=None):
        """
        –ü–∞—Ä—Å–∏–Ω–≥ Excel —Ñ–∞–π–ª–∞
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            sheet_name: –ò–º—è –ª–∏—Å—Ç–∞ (–µ—Å–ª–∏ None - –ø–µ—Ä–≤—ã–π –ª–∏—Å—Ç –∏–ª–∏ –ø–æ–∏—Å–∫ –ø–æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏)
            subcategory_name: –ù–∞–∑–≤–∞–Ω–∏–µ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ª–∏—Å—Ç–∞
        """
        try:
            # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–≤–∏–∂–æ–∫
            if file_path.suffix == '.xlsx':
                engine = 'openpyxl'
            else:
                engine = 'xlrd'
            
            # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –∏–º—è –ª–∏—Å—Ç–∞ - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ
            if sheet_name:
                df = pd.read_excel(file_path, sheet_name=sheet_name, engine=engine)
            elif subcategory_name:
                # –ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –Ω–∞–π—Ç–∏ –ª–∏—Å—Ç –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                excel_file = pd.ExcelFile(file_path, engine=engine)
                found_sheet = None
                
                # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                if subcategory_name in excel_file.sheet_names:
                    found_sheet = subcategory_name
                else:
                    # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–ø–µ—Ä–≤—ã–µ 31 —Å–∏–º–≤–æ–ª - –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ Excel)
                    subcat_short = subcategory_name[:31]
                    for sheet in excel_file.sheet_names:
                        if subcat_short in sheet or sheet in subcategory_name:
                            found_sheet = sheet
                            break
                
                if found_sheet:
                    df = pd.read_excel(file_path, sheet_name=found_sheet, engine=engine)
                else:
                    # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–≤—ã–π –ª–∏—Å—Ç (–ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏)
                    available_sheets = [s for s in excel_file.sheet_names if not s.startswith('üìã') and s != '–ò–ù–°–¢–†–£–ö–¶–ò–Ø']
                    if available_sheets:
                        df = pd.read_excel(file_path, sheet_name=available_sheets[0], engine=engine)
                    else:
                        df = pd.read_excel(file_path, engine=engine)
            else:
                # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–≤—ã–π –ª–∏—Å—Ç (–ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏)
                excel_file = pd.ExcelFile(file_path, engine=engine)
                available_sheets = [s for s in excel_file.sheet_names if not s.startswith('üìã') and s != '–ò–ù–°–¢–†–£–ö–¶–ò–Ø']
                if available_sheets:
                    df = pd.read_excel(file_path, sheet_name=available_sheets[0], engine=engine)
                else:
                    df = pd.read_excel(file_path, engine=engine)
            
            # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Å—Ç—Ä–æ–∫—É —Å –ø—Ä–∏–º–µ—Ä–æ–º (–µ—Å–ª–∏ –µ—Å—Ç—å)
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É –¥–∞–Ω–Ω—ã—Ö
            if len(df) > 0:
                first_row = df.iloc[0]
                # –ï—Å–ª–∏ –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç "–ü–†–ò–ú–ï–†" - –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –µ—ë
                if any('–ü–†–ò–ú–ï–†' in str(val).upper() for val in first_row.values if pd.notna(val)):
                    df = df.iloc[1:].reset_index(drop=True)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
            data = df.to_dict('records')
            
            # –û—á–∏—Å—Ç–∏—Ç—å NaN –∑–Ω–∞—á–µ–Ω–∏—è
            for row in data:
                for key, value in row.items():
                    try:
                        if pd.isna(value):
                            row[key] = None
                    except (TypeError, ValueError):
                        # –ï—Å–ª–∏ –Ω–µ pandas –∑–Ω–∞—á–µ–Ω–∏–µ, –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ None
                        if value is None:
                            row[key] = None
            
            total_rows = len(data)
            return data, total_rows
        except Exception as e:
            raise ValueError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ Excel —Ñ–∞–π–ª–∞: {str(e)}")
    
    @staticmethod
    def _parse_csv(file_path):
        """–ü–∞—Ä—Å–∏–Ω–≥ CSV —Ñ–∞–π–ª–∞"""
        try:
            # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
            encodings = ['utf-8', 'cp1251', 'windows-1251', 'latin-1']
            data = None
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
                        sample = f.read(1024)
                        f.seek(0)
                        sniffer = csv.Sniffer()
                        delimiter = sniffer.sniff(sample).delimiter
                        
                        reader = csv.DictReader(f, delimiter=delimiter)
                        data = list(reader)
                        break
                except UnicodeDecodeError:
                    continue
            
            if data is None:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É CSV —Ñ–∞–π–ª–∞")
            
            total_rows = len(data)
            return data, total_rows
        except Exception as e:
            raise ValueError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ CSV —Ñ–∞–π–ª–∞: {str(e)}")
    
    @staticmethod
    def _parse_json(file_path):
        """–ü–∞—Ä—Å–∏–Ω–≥ JSON —Ñ–∞–π–ª–∞"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π - –≤–µ—Ä–Ω—É—Ç—å –∫–∞–∫ –µ—Å—Ç—å
            if isinstance(data, list):
                total_rows = len(data)
                return data, total_rows
            
            # –ï—Å–ª–∏ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–æ–º 'products' –∏–ª–∏ 'items'
            if isinstance(data, dict):
                if 'products' in data:
                    result_data = data['products']
                    total_rows = len(result_data) if isinstance(result_data, list) else 1
                    return result_data, total_rows
                elif 'items' in data:
                    result_data = data['items']
                    total_rows = len(result_data) if isinstance(result_data, list) else 1
                    return result_data, total_rows
                else:
                    # –í–µ—Ä–Ω—É—Ç—å –∫–∞–∫ —Å–ø–∏—Å–æ–∫ —Å –æ–¥–Ω–∏–º —ç–ª–µ–º–µ–Ω—Ç–æ–º
                    return [data], 1
            
            raise ValueError("–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç JSON —Ñ–∞–π–ª–∞")
        except json.JSONDecodeError as e:
            raise ValueError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ JSON: {str(e)}")
    
    @staticmethod
    def _import_products(data, subcategory_id, user=None, auto_verify=True, import_history_id=None):
        """
        –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–≤–∞—Ä—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            data: –î–∞–Ω–Ω—ã–µ –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
            subcategory_id: ID –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            user: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
            auto_verify: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è
            import_history_id: ID –∑–∞–ø–∏—Å–∏ ImportHistory (–¥–ª—è —Å–≤—è–∑–∏ —Ç–æ–≤–∞—Ä–æ–≤ —Å —Ñ–∞–π–ª–æ–º)
        
        Returns:
            dict: {
                'imported': –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö,
                'errors': —Å–ø–∏—Å–æ–∫ –æ—à–∏–±–æ–∫,
                'warnings': —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π,
                'products': —Å–ø–∏—Å–æ–∫ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤
            }
        """
        subcategory = Subcategory.query.get_or_404(subcategory_id)
        
        imported_count = 0
        errors = []
        warnings = []
        products = []
        
        # –ü–æ–ª—É—á–∏—Ç—å —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        reference_attributes = {attr.attribute.code: attr for attr in subcategory.get_all_attributes()}
        
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–∞–ø–ø–∏–Ω–≥ –ø–æ–ª–µ–π (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π)
        if not data:
            return {
                'imported': 0,
                'errors': ['–§–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã—Ö'],
                'warnings': [],
                'products': []
            }
        
        # –ü–æ–ª—É—á–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∏–∑ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–∏
        first_row = data[0]
        column_mapping = ImportService._auto_map_fields(first_row.keys(), reference_attributes.keys())
        
        # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞–∂–¥—É—é —Å—Ç—Ä–æ–∫—É
        for row_num, row_data in enumerate(data, start=2):  # –ù–∞—á–∏–Ω–∞–µ–º —Å 2 (–ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ - –∑–∞–≥–æ–ª–æ–≤–∫–∏)
            try:
                # –°–æ–∑–¥–∞—Ç—å —Ç–æ–≤–∞—Ä
                product = ImportService._create_product_from_row(
                    row_data, 
                    subcategory, 
                    column_mapping, 
                    reference_attributes,
                    user
                )
                
                products.append(product)
                imported_count += 1
                
                # –°–∫–∞—á–∞—Ç—å –º–µ–¥–∏–∞-—Ñ–∞–π–ª—ã (—Ñ–æ—Ç–æ –∏ 3D –º–æ–¥–µ–ª–∏)
                try:
                    from app.services.media_service import MediaService
                    media_stats = MediaService.process_product_media(product, auto_download=True)
                    if media_stats['images_downloaded'] > 0:
                        warnings.append(f"–°—Ç—Ä–æ–∫–∞ {row_num}: –°–∫–∞—á–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {media_stats['images_downloaded']}")
                    if media_stats['models_downloaded'] > 0:
                        warnings.append(f"–°—Ç—Ä–æ–∫–∞ {row_num}: –°–∫–∞—á–∞–Ω–æ 3D –º–æ–¥–µ–ª–µ–π: {media_stats['models_downloaded']}")
                    if media_stats['errors']:
                        for error in media_stats['errors']:
                            warnings.append(f"–°—Ç—Ä–æ–∫–∞ {row_num}: {error}")
                except Exception as e:
                    warnings.append(f"–°—Ç—Ä–æ–∫–∞ {row_num}: –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –º–µ–¥–∏–∞-—Ñ–∞–π–ª–æ–≤ - {str(e)}")
                
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è
                if auto_verify:
                    try:
                        VerificationService.verify_product(product, user)
                    except Exception as e:
                        warnings.append(f"–°—Ç—Ä–æ–∫–∞ {row_num}: –û—à–∏–±–∫–∞ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ - {str(e)}")
                
            except Exception as e:
                errors.append(f"–°—Ç—Ä–æ–∫–∞ {row_num}: {str(e)}")
                continue
        
        db.session.commit()
        
        return {
            'imported': imported_count,
            'total_rows': total_rows,
            'errors': errors,
            'warnings': warnings,
            'products': [p.to_dict() for p in products]
        }
    
    @staticmethod
    def _auto_map_fields(file_columns, attribute_codes):
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–∞–ø–ø–∏–Ω–≥ –ø–æ–ª–µ–π —Ñ–∞–π–ª–∞ —Å –∞—Ç—Ä–∏–±—É—Ç–∞–º–∏
        
        Args:
            file_columns: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫ –≤ —Ñ–∞–π–ª–µ
            attribute_codes: –°–ø–∏—Å–æ–∫ –∫–æ–¥–æ–≤ –∞—Ç—Ä–∏–±—É—Ç–æ–≤
        
        Returns:
            dict: –ú–∞–ø–ø–∏–Ω–≥ {–Ω–∞–∑–≤–∞–Ω–∏–µ_–∫–æ–ª–æ–Ω–∫–∏: –∫–æ–¥_–∞—Ç—Ä–∏–±—É—Ç–∞}
        """
        mapping = {}
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è (–Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, —É–±—Ä–∞—Ç—å –ø—Ä–æ–±–µ–ª—ã)
        normalized_columns = {col.lower().strip(): col for col in file_columns}
        normalized_attributes = {code.lower().strip(): code for code in attribute_codes}
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏ –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –ø–æ–ª–µ–π
        special_mappings = {
            'sku': ['sku', '–∞—Ä—Ç–∏–∫—É–ª', 'article', '–∫–æ–¥', 'code'],
            'name': ['name', '–Ω–∞–∑–≤–∞–Ω–∏–µ', 'title', '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ'],
            'description': ['description', '–æ–ø–∏—Å–∞–Ω–∏–µ', 'desc'],
        }
        
        # –ú–∞–ø–ø–∏–Ω–≥ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –ø–æ–ª–µ–π
        for attr_code, possible_names in special_mappings.items():
            if attr_code in normalized_attributes:
                for name in possible_names:
                    if name in normalized_columns:
                        mapping[normalized_columns[name]] = attr_code
                        break
        
        # –ú–∞–ø–ø–∏–Ω–≥ –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –ø–æ —Ç–æ—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é
        for norm_col, orig_col in normalized_columns.items():
            if norm_col in normalized_attributes:
                if orig_col not in mapping:  # –ù–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞—Ç—å —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ
                    mapping[orig_col] = normalized_attributes[norm_col]
        
        # –ú–∞–ø–ø–∏–Ω–≥ –ø–æ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é
        for norm_col, orig_col in normalized_columns.items():
            if orig_col not in mapping:
                for norm_attr, attr_code in normalized_attributes.items():
                    if norm_attr in norm_col or norm_col in norm_attr:
                        mapping[orig_col] = attr_code
                        break
        
        return mapping
    
    @staticmethod
    def _create_product_from_row(row_data, subcategory, column_mapping, reference_attributes, user):
        """
        –°–æ–∑–¥–∞—Ç—å —Ç–æ–≤–∞—Ä –∏–∑ —Å—Ç—Ä–æ–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            row_data: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ —Å—Ç—Ä–æ–∫–∏
            subcategory: –û–±—ä–µ–∫—Ç Subcategory
            column_mapping: –ú–∞–ø–ø–∏–Ω–≥ –∫–æ–ª–æ–Ω–æ–∫ –Ω–∞ –∫–æ–¥—ã –∞—Ç—Ä–∏–±—É—Ç–æ–≤
            reference_attributes: –°–ª–æ–≤–∞—Ä—å —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤ {code: SubcategoryAttribute}
            user: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
        
        Returns:
            Product: –°–æ–∑–¥–∞–Ω–Ω—ã–π —Ç–æ–≤–∞—Ä
        """
        # –ü–æ–ª—É—á–∏—Ç—å SKU (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ)
        sku = None
        for col_name, attr_code in column_mapping.items():
            if attr_code == 'sku' and col_name in row_data:
                sku = str(row_data[col_name]).strip() if row_data[col_name] is not None else None
                break
        
        # –ï—Å–ª–∏ SKU –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–∞–π—Ç–∏ –≤ –ª—é–±–æ–º –ø–æ–ª–µ
        if not sku:
            for key, value in row_data.items():
                if value and ('sku' in key.lower() or '–∞—Ä—Ç–∏–∫—É–ª' in key.lower() or '–∫–æ–¥' in key.lower()):
                    sku = str(value).strip()
                    break
        
        if not sku:
            raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω –∞—Ä—Ç–∏–∫—É–ª (SKU) —Ç–æ–≤–∞—Ä–∞")
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å SKU
        if Product.query.filter_by(sku=sku).first():
            raise ValueError(f"–¢–æ–≤–∞—Ä —Å –∞—Ä—Ç–∏–∫—É–ª–æ–º {sku} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è (–µ—Å–ª–∏ –µ—Å—Ç—å —Ç–∞–∫–æ–π –∞—Ç—Ä–∏–±—É—Ç)
        manufacturer_sku = None
        manufacturer_sku_attr_code = None
        
        # –ò—â–µ–º –∞—Ç—Ä–∏–±—É—Ç –∞—Ä—Ç–∏–∫—É–ª–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è
        for col_name, attr_code in column_mapping.items():
            if attr_code in ['manufacturer_sku', 'manufacturer_code', 'manufacturer_article', 
                            '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å_–∞—Ä—Ç–∏–∫—É–ª', '–∞—Ä—Ç–∏–∫—É–ª_–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è']:
                if col_name in row_data and row_data[col_name]:
                    manufacturer_sku = str(row_data[col_name]).strip()
                    manufacturer_sku_attr_code = attr_code
                    break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ –º–∞–ø–ø–∏–Ω–≥–µ, –∏—â–µ–º –≤ –¥–∞–Ω–Ω—ã—Ö
        if not manufacturer_sku:
            for key, value in row_data.items():
                key_lower = key.lower()
                if value and ('manufacturer' in key_lower or '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å' in key_lower) and \
                   ('sku' in key_lower or 'code' in key_lower or 'article' in key_lower or '–∞—Ä—Ç–∏–∫—É–ª' in key_lower):
                    manufacturer_sku = str(value).strip()
                    break
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –∞—Ä—Ç–∏–∫—É–ª—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è
        if manufacturer_sku:
            from app.models.attribute import Attribute
            from app.models.product import ProductAttributeValue
            
            # –ù–∞–π—Ç–∏ –∞—Ç—Ä–∏–±—É—Ç –∞—Ä—Ç–∏–∫—É–ª–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è
            manufacturer_attr = Attribute.query.filter(
                Attribute.code.in_(['manufacturer_sku', 'manufacturer_code', 'manufacturer_article'])
            ).first()
            
            if manufacturer_attr:
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–æ–≤–∞—Ä —Å —Ç–∞–∫–∏–º –∞—Ä—Ç–∏–∫—É–ª–æ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è
                duplicate_pav = ProductAttributeValue.query.filter(
                    ProductAttributeValue.attribute_id == manufacturer_attr.id,
                    ProductAttributeValue.value == manufacturer_sku
                ).first()
                
                if duplicate_pav:
                    raise ValueError(f"–¢–æ–≤–∞—Ä —Å –∞—Ä—Ç–∏–∫—É–ª–æ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è {manufacturer_sku} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç (—Ç–æ–≤–∞—Ä ID: {duplicate_pav.product_id})")
        
        # –ü–æ–ª—É—á–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ
        name = None
        for col_name, attr_code in column_mapping.items():
            if attr_code == 'name' and col_name in row_data:
                name = str(row_data[col_name]).strip() if row_data[col_name] is not None else None
                break
        
        if not name:
            # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–∞–π—Ç–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤ –ª—é–±–æ–º –ø–æ–ª–µ
            for key, value in row_data.items():
                if value and ('name' in key.lower() or '–Ω–∞–∑–≤–∞–Ω–∏–µ' in key.lower() or 'title' in key.lower()):
                    name = str(value).strip()
                    break
        
        if not name:
            name = f"–¢–æ–≤–∞—Ä {sku}"  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SKU –∫–∞–∫ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # –°–æ–∑–¥–∞—Ç—å —Ç–æ–≤–∞—Ä
        product = Product(
            sku=sku,
            name=name,
            subcategory_id=subcategory.id,
            status=ProductStatus.DRAFT,
            created_by_id=user.id if user else None,
            import_history_id=import_history_id  # –°–≤—è–∑—å —Å —Ñ–∞–π–ª–æ–º –∏–º–ø–æ—Ä—Ç–∞
        )
        db.session.add(product)
        
        # –ó–∞–ø–∏—Å–∞—Ç—å –ø–µ—Ä–µ—Ö–æ–¥ –≤ —Å—Ç–∞—Ç—É—Å in_progress
        history = ProductStatusHistory(
            product_id=product.id,
            old_status=ProductStatus.DRAFT.value,
            new_status=ProductStatus.IN_PROGRESS.value,
            changed_by_id=user.id if user else None,
            comment='–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ—Ö–æ–¥ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ'
        )
        product.status = ProductStatus.IN_PROGRESS
        db.session.add(history)
        
        # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∞—Ç—Ä–∏–±—É—Ç—ã
        for col_name, value in row_data.items():
            if value is None:
                continue
            try:
                if hasattr(pd, 'isna') and pd.isna(value):
                    continue
            except (TypeError, ValueError):
                pass
            
            # –ù–∞–π—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∞—Ç—Ä–∏–±—É—Ç
            attr_code = column_mapping.get(col_name)
            if not attr_code or attr_code not in reference_attributes:
                continue  # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª—è, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –º–∞–ø–ø—è—Ç—Å—è –Ω–∞ —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
            
            subcat_attr = reference_attributes[attr_code]
            attribute = subcat_attr.attribute
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É
            str_value = str(value).strip()
            if not str_value:
                continue
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö
            if not ImportService._validate_attribute_value(attribute, str_value):
                continue  # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            
            # –°–æ–∑–¥–∞—Ç—å –∏–ª–∏ –æ–±–Ω–æ–≤–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–∞
            pav = ProductAttributeValue.query.filter_by(
                product_id=product.id,
                attribute_id=attribute.id
            ).first()
            
            if pav:
                pav.value = str_value
            else:
                pav = ProductAttributeValue(
                    product_id=product.id,
                    attribute_id=attribute.id,
                    value=str_value
                )
                db.session.add(pav)
        
        db.session.flush()  # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ë–î –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è ID
        
        return product
    
    @staticmethod
    def _validate_attribute_value(attribute, value):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–∏–ø—É"""
        try:
            if attribute.type == AttributeType.NUMBER:
                float(value)
            elif attribute.type == AttributeType.BOOLEAN:
                value.lower() in ['true', 'false', '1', '0', 'yes', 'no', '–¥–∞', '–Ω–µ—Ç']
            elif attribute.type == AttributeType.DATE:
                # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç—ã
                from dateutil import parser
                parser.parse(value)
            elif attribute.type == AttributeType.SELECT:
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ –µ—Å—Ç—å –≤ —Å–ø–∏—Å–∫–µ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö
                allowed_values = [av.value for av in attribute.values.all()]
                return value in allowed_values
        except (ValueError, TypeError):
            return False
        
        return True

